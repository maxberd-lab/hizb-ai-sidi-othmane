import streamlit as st
from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage

# --- CONFIGURATION DE LA PAGE ---
st.set_page_config(
    page_title="Hizb Sidi Othmane AI", 
    page_icon="üá≤üá¶", 
    layout="centered"
)

# --- STYLE PERSONNALIS√â (Optionnel pour faire "Pro") ---
st.markdown("""
    <style>
    .main { background-color: #f5f7f9; }
    .stButton>button { width: 100%; border-radius: 20px; border: none; }
    </style>
    """, unsafe_allow_request_usage=True)

st.title("ü§ñ Assistant Digital - Sidi Othmane")
st.subheader("Programme √âlectoral & Citoyennet√©")

# --- 1. GESTION DE LA CL√â API ---
# On v√©rifie d'abord les Secrets Streamlit, sinon on demande une saisie manuelle
if "GROQ_API_KEY" in st.secrets:
    api_key = st.secrets["GROQ_API_KEY"]
else:
    api_key = st.sidebar.text_input("Cl√© API Groq (Optionnel si Secrets configur√©s)", type="password")

if not api_key:
    st.info("üí° En attente de la configuration de la cl√© API...")
    st.stop()

# --- 2. CHARGEMENT DES DONN√âES DE CONNAISSANCE ---
try:
    with open("data.txt", "r", encoding="utf-8") as f:
        connaissances_locales = f.read()
except FileNotFoundError:
    st.error("‚ùå Erreur : Le fichier 'data.txt' est introuvable sur votre GitHub.")
    st.stop()

# --- 3. INITIALISATION DU MOD√àLE ET DE L'HISTORIQUE ---
# Utilisation de Llama 3.3 70B (Mod√®le actuel le plus performant sur Groq)
llm = ChatGroq(
    temperature=0.3, 
    groq_api_key=api_key, 
    model_name="llama-3.3-70b-versatile"
)

if "messages" not in st.session_state:
    st.session_state.messages = []

# Affichage des messages de la discussion
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 4. INTERACTION UTILISATEUR ---
if prompt := st.chat_input("Posez votre question sur Sidi Othmane..."):
    # Ajouter le message de l'utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Pr√©paration du message syst√®me (Instructions)
    instruction_politique = f"""
    Tu es l'expert digital du parti pour l'arrondissement de Sidi Othmane. 
    Tes r√©ponses doivent √™tre bas√©es EXCLUSIVEMENT sur ces informations : {connaissances_locales}.
    Si l'utilisateur pose une question hors sujet, r√©ponds poliment que tu es l√† pour parler du programme local.
    R√©ponds en Darija si l'utilisateur t'√©crit en Darija. Sois encourageant et patriotique.
    """

    # G√©n√©ration de la r√©ponse
    with st.chat_message("assistant"):
        try:
            with st.spinner("R√©flexion en cours..."):
                full_prompt = [
                    SystemMessage(content=instruction_politique),
                    HumanMessage(content=prompt)
                ]
                response = llm.invoke(full_prompt)
                full_response = response.content
                st.markdown(full_response)
                
                # Sauvegarder la r√©ponse dans l'historique
                st.session_state.messages.append({"role": "assistant", "content": full_response})
        except Exception as e:
            st.error(f"D√©sol√©, une erreur technique est survenue : {e}")